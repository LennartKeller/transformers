{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "from turtle import forward\n",
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable TopK-Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each layer only $k$ token-embeddings are selected and passed to the next layer.\n",
    "\n",
    "To make this operation differentiable it cannot use a hard selection scheme.\n",
    "\n",
    "The authors propose a an algorithm which uses two steps:\n",
    "\n",
    "1. Sorting\n",
    "   * The embeddings passed into the current layer are scored using a linear operation (e.g. dot product)\n",
    "   * The embeddings get sorted according to their score\n",
    "   * Question: Sorting is a non-differentiable or partially non-differentiable?\n",
    "   * Note: After selection the embeddings are re-arranged according to their original order????? This does not make any sense since they are positionally encoded???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 1000, (16, 512, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[123,  71,  97,  ...,  88,  24,  85],\n",
       "         [ 96, 103,  73,  ...,  46,  66, 113],\n",
       "         [104,  36,  55,  ..., 105,  22,  89],\n",
       "         ...,\n",
       "         [113,  18,  39,  ...,  67,  45,  32],\n",
       "         [ 32,  13,  10,  ...,  77,   9,  56],\n",
       "         [ 98,  49,  57,  ...,  52,  10,  19]],\n",
       "\n",
       "        [[ 19, 107, 127,  ...,  45,  28,  70],\n",
       "         [ 42,  48,  97,  ..., 100,  86,  17],\n",
       "         [ 48,  43,  98,  ...,  80,  55,  96],\n",
       "         ...,\n",
       "         [ 70,  22,  10,  ...,  28,  85,  73],\n",
       "         [108,  37,   7,  ...,   0, 125,  24],\n",
       "         [ 47,  86, 121,  ...,  54,   9,   0]],\n",
       "\n",
       "        [[127, 125,   1,  ...,  16,   7,  50],\n",
       "         [114,  65,  84,  ...,  96,  11,  13],\n",
       "         [ 73,  54, 106,  ...,  20,  24,  25],\n",
       "         ...,\n",
       "         [ 65,  12,  15,  ...,  54,  40,   4],\n",
       "         [ 61,  33,  64,  ..., 110, 126,  29],\n",
       "         [ 89,  53,  60,  ...,  71,  28,  95]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 72,  66,  34,  ...,  77,  70,  84],\n",
       "         [ 91, 125, 114,  ..., 102,   8,  98],\n",
       "         [ 87,  34,  96,  ...,   0,  52,  60],\n",
       "         ...,\n",
       "         [ 86,  62,  27,  ...,  29,   2,  81],\n",
       "         [ 89, 109,  14,  ...,   8,  44,  10],\n",
       "         [ 76,  14,  91,  ...,  78, 109,  87]],\n",
       "\n",
       "        [[116,  40,  24,  ...,  32, 108,  13],\n",
       "         [109,  75,  48,  ...,  44,  51, 101],\n",
       "         [ 15,  43,  76,  ...,  70,  98,  48],\n",
       "         ...,\n",
       "         [ 65,  58,  45,  ..., 107,  98,  70],\n",
       "         [125,  37,  50,  ...,  24,  62,   4],\n",
       "         [ 85,  89,  13,  ...,  66, 109,  65]],\n",
       "\n",
       "        [[115,  39,  77,  ...,  66,  70,  40],\n",
       "         [ 61, 123, 127,  ...,  78,  41,   8],\n",
       "         [ 27,  66,  79,  ...,  47,  15,  54],\n",
       "         ...,\n",
       "         [ 52,  68,   1,  ..., 116, 119,  23],\n",
       "         [  2,  44,  32,  ...,  49, 113,  63],\n",
       "         [106,  78, 126,  ...,  61,  94,  46]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(x).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaked_softmax(x: Tensor, alpha: float = 0.1, dim: int = 0):\n",
    "    x = x * alpha\n",
    "    softmax = nn.Softmax(dim=dim)\n",
    "    return softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.randn(10)\n",
    "for i in torch.linspace(1, 3, 10):\n",
    "    plt.plot(peaked_softmax(x, alpha=i.item()).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKPooler(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings: Tensor):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            embeddings (Tensor): tensor of shape (batch_size, n_embeddings,\n",
    "        \"\"\"\n",
    "        # 1. Score embeddings\n",
    "        scores = self.scorer(embeddings)\n",
    "        \n",
    "        # 2. Sort embeddings according to their score in descending order\n",
    "        embeddings_sorted, scores_sorted = self.sort_embeddings(embeddings=embeddings, scores=scores)\n",
    "\n",
    "        # 3. Create new tensors with pairs of embeddings and scores\n",
    "        # Create embeddings pairs\n",
    "\n",
    "        middle_idx = int(embeddings.size(1) // 2)\n",
    "        #print(middle_idx)\n",
    "\n",
    "        left_embeddings, right_embeddings = embeddings_sorted[:, :middle_idx, :], embeddings_sorted[:, middle_idx:, :]\n",
    "        embedding_pairs = torch.cat((left_embeddings.unsqueeze(3), right_embeddings.flip(1).unsqueeze(3)), dim=3)\n",
    "        #print(embedding_pairs.size())  # shape: bs, seqlen // 2, n_dim, 2\n",
    "\n",
    "        # Create score pairs\n",
    "\n",
    "        left_scores, right_scores = scores_sorted[:, :middle_idx, :], scores_sorted[:, middle_idx:, :]\n",
    "        score_pairs = torch.cat((left_scores.unsqueeze(3), right_scores.flip(1).unsqueeze(3)), dim=3)\n",
    "        #print(score_pairs.size())  # shape: bs, seqlen // 2, 1, 2\n",
    "\n",
    "        score_pairs_softmaxed = peaked_softmax(score_pairs, alpha=5.0, dim=3)\n",
    "        #print(score_pairs_softmaxed.size())\n",
    "\n",
    "        new_embeddings = (embedding_pairs * score_pairs_softmaxed).sum(dim=3)\n",
    "        #print(new_embeddings.size())\n",
    "        return new_embeddings    \n",
    "    \n",
    "    def sort_embeddings(self, embeddings: Tensor, scores: Tensor):\n",
    "        \"\"\"Sorts the embeddings and score according to their score in descending order\n",
    "\n",
    "        Args:\n",
    "            embeddings (Tensor): tensor of shape (batch_size, n_embeddings, n_dims)\n",
    "            scores (Tensor): (batch_size, n_embeddings, 1)\n",
    "\n",
    "        Returns:\n",
    "            _type_: Sorted embeddings and scores\n",
    "        \"\"\"\n",
    "        sort_idc = scores.sort(descending=True, dim=1).indices\n",
    "        #print(sort_idc.size())\n",
    "        \n",
    "        embeddings_sorted = self.sort_by_indices(embeddings, sort_idc)  #???\n",
    "        #print(embeddings_sorted.size())\n",
    "        \n",
    "        scores_sorted = self.sort_by_indices(scores, sort_idc)  #???\n",
    "        #print(scores_sorted.size())\n",
    "        \n",
    "        return embeddings_sorted, scores_sorted\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort_by_indices(embeddings, indices):\n",
    "        sorted_batches = []\n",
    "        for batch_embeddings, batch_scores in zip(embeddings, indices):\n",
    "            sorted_batches.append(batch_embeddings[batch_scores.reshape(-1)].unsqueeze(0))\n",
    "        return torch.cat(sorted_batches, dim=0)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (1): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (2): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (3): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (4): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (5): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (6): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (7): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (8): TopKPooler(\n",
       "    (scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = torch.log2(torch.tensor(512)).long().item()\n",
    "\n",
    "pyramid = nn.Sequential(*(\n",
    "    TopKPooler(embedding_dim=768)\n",
    "    for _ in range(n_layers)\n",
    "))\n",
    "pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 768])\n",
      "torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "src = torch.randn(8, 512, 768)\n",
    "trgt = src[:, 64, :]\n",
    "print(src.size())\n",
    "print(trgt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pyramid(src)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9736: 100%|██████████| 10/10 [00:00<00:00, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.SGD(params=pyramid.parameters(), lr=0.1)\n",
    "pbar = tqdm(list(range(10)))\n",
    "for _ in pbar:\n",
    "    out = pyramid(src).squeeze(1)\n",
    "    loss_fct = nn.MSELoss()\n",
    "    loss = loss_fct(out, trgt)\n",
    "    pbar.set_description(f\"Loss: {round(loss.item(), 4)}\")\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0054,  0.0102, -0.0646,  ...,  0.0714, -0.0505,  0.0976],\n",
      "        [ 0.0732,  0.0536, -0.0883,  ...,  0.0018,  0.0057, -0.0369],\n",
      "        [ 0.0680, -0.0166,  0.0343,  ...,  0.1024, -0.0593, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0277,  0.0488, -0.0600,  ..., -0.0671,  0.1369,  0.0392],\n",
      "        [-0.0804,  0.0356,  0.0194,  ..., -0.0242,  0.0181, -0.0136],\n",
      "        [ 0.0296,  0.0487, -0.0442,  ..., -0.0638, -0.0286,  0.1441]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([[-2.1768e-01,  2.1552e-01, -3.8509e-01,  ...,  1.7892e+00,\n",
      "          3.0278e-01, -5.2488e-01],\n",
      "        [-3.1580e-02,  1.2799e+00, -8.3881e-01,  ..., -6.6179e-01,\n",
      "         -6.9448e-01, -1.3409e+00],\n",
      "        [-1.2001e-01,  7.4544e-02, -2.0118e-01,  ..., -8.8894e-01,\n",
      "          8.9552e-02, -3.7010e-01],\n",
      "        ...,\n",
      "        [-1.4209e+00,  2.4367e-01,  6.5688e-01,  ...,  8.7199e-01,\n",
      "          3.3947e-01, -5.0424e-01],\n",
      "        [-2.1058e+00,  1.0508e+00,  2.1872e+00,  ..., -1.6396e+00,\n",
      "         -1.0057e+00, -2.8931e-01],\n",
      "        [ 4.8227e-04, -1.4010e-01, -9.6459e-01,  ...,  4.1072e-01,\n",
      "          3.6692e-01, -2.6483e-01]])\n"
     ]
    }
   ],
   "source": [
    "out = pyramid(src).squeeze(1)\n",
    "print(out)\n",
    "print(trgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1768e-01,  2.1552e-01, -3.8509e-01,  ...,  1.7892e+00,\n",
       "          3.0278e-01, -5.2488e-01],\n",
       "        [-3.1580e-02,  1.2799e+00, -8.3881e-01,  ..., -6.6179e-01,\n",
       "         -6.9448e-01, -1.3409e+00],\n",
       "        [-1.2001e-01,  7.4544e-02, -2.0118e-01,  ..., -8.8894e-01,\n",
       "          8.9552e-02, -3.7010e-01],\n",
       "        ...,\n",
       "        [-1.4209e+00,  2.4367e-01,  6.5688e-01,  ...,  8.7199e-01,\n",
       "          3.3947e-01, -5.0424e-01],\n",
       "        [-2.1058e+00,  1.0508e+00,  2.1872e+00,  ..., -1.6396e+00,\n",
       "         -1.0057e+00, -2.8931e-01],\n",
       "        [ 4.8227e-04, -1.4010e-01, -9.6459e-01,  ...,  4.1072e-01,\n",
       "          3.6692e-01, -2.6483e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(16, 512, 1).squeeze(2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "e = torch.tensor([\n",
    "    [\n",
    "        [0.1, 1.9],\n",
    "        [0.0, 0.5],\n",
    "        [0.7, 0.9]\n",
    "    ],\n",
    "    [\n",
    "        [0.0, 0.0],\n",
    "        [0.0, 0.5],\n",
    "        [0.7, 0.9]\n",
    "    ]\n",
    "])\n",
    "print(e.size())\n",
    "\n",
    "s = torch.tensor([\n",
    "    [\n",
    "        [0],\n",
    "        [2],\n",
    "        [1]\n",
    "    ],\n",
    "    [\n",
    "        [2],\n",
    "        [1],\n",
    "        [0]\n",
    "    ],\n",
    "])\n",
    "print(s.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).repeat(2).reshape(2, -1).flip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "* Pyramdion only serves as encoder model?\n",
    "* Which architecture to use for decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PyramidionsConfig, PyramidionsModel, RobertaTokenizer, PyramidionsForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /Users/lennartkeller/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /Users/lennartkeller/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/lennartkeller/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(\n",
    "            1 + 1, 512 + 1 + 1, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,\n",
       "         16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "         30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
       "         44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,\n",
       "         58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
       "         72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,\n",
       "         86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
       "        100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
       "        114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127,\n",
       "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
       "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
       "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
       "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "        254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267,\n",
       "        268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
       "        282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "        296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
       "        310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "        324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "        352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
       "        366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379,\n",
       "        380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393,\n",
       "        394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407,\n",
       "        408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
       "        422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n",
       "        436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449,\n",
       "        450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463,\n",
       "        464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "        478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "        492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505,\n",
       "        506, 507, 508, 509, 510, 511, 512, 513])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyramidionsConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"pyramidions\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 9,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = PyramidionsConfig()\n",
    "config.update({\"num_hidden_layers\": 9, \"max_position_embeddings\": 514, \"type_vocab_size\": 2, \"type_vocab_size\": 1})\n",
    "print(config)\n",
    "model = PyramidionsForSequenceClassification(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1202, -0.4388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokenizer(\"Hallo\", padding=\"max_length\", return_tensors=\"pt\"))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/lennartkeller/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /Users/lennartkeller/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "\n",
    "pretrained_model = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def rename_roberta_state_dict(state_dict):\n",
    "    return OrderedDict([(f\"pyramidions.{key}\", value) for key, value in state_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['pyramidions.encoder.layer.0.pooler.scorer.0.weight', 'pyramidions.encoder.layer.0.pooler.scorer.0.bias', 'pyramidions.encoder.layer.1.pooler.scorer.0.weight', 'pyramidions.encoder.layer.1.pooler.scorer.0.bias', 'pyramidions.encoder.layer.2.pooler.scorer.0.weight', 'pyramidions.encoder.layer.2.pooler.scorer.0.bias', 'pyramidions.encoder.layer.3.pooler.scorer.0.weight', 'pyramidions.encoder.layer.3.pooler.scorer.0.bias', 'pyramidions.encoder.layer.4.pooler.scorer.0.weight', 'pyramidions.encoder.layer.4.pooler.scorer.0.bias', 'pyramidions.encoder.layer.5.pooler.scorer.0.weight', 'pyramidions.encoder.layer.5.pooler.scorer.0.bias', 'pyramidions.encoder.layer.6.pooler.scorer.0.weight', 'pyramidions.encoder.layer.6.pooler.scorer.0.bias', 'pyramidions.encoder.layer.7.pooler.scorer.0.weight', 'pyramidions.encoder.layer.7.pooler.scorer.0.bias', 'pyramidions.encoder.layer.8.pooler.scorer.0.weight', 'pyramidions.encoder.layer.8.pooler.scorer.0.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'], unexpected_keys=['pyramidions.pooler.dense.weight', 'pyramidions.pooler.dense.bias', 'pyramidions.encoder.layer.9.attention.self.query.weight', 'pyramidions.encoder.layer.9.attention.self.query.bias', 'pyramidions.encoder.layer.9.attention.self.key.weight', 'pyramidions.encoder.layer.9.attention.self.key.bias', 'pyramidions.encoder.layer.9.attention.self.value.weight', 'pyramidions.encoder.layer.9.attention.self.value.bias', 'pyramidions.encoder.layer.9.attention.output.dense.weight', 'pyramidions.encoder.layer.9.attention.output.dense.bias', 'pyramidions.encoder.layer.9.attention.output.LayerNorm.weight', 'pyramidions.encoder.layer.9.attention.output.LayerNorm.bias', 'pyramidions.encoder.layer.9.intermediate.dense.weight', 'pyramidions.encoder.layer.9.intermediate.dense.bias', 'pyramidions.encoder.layer.9.output.dense.weight', 'pyramidions.encoder.layer.9.output.dense.bias', 'pyramidions.encoder.layer.9.output.LayerNorm.weight', 'pyramidions.encoder.layer.9.output.LayerNorm.bias', 'pyramidions.encoder.layer.10.attention.self.query.weight', 'pyramidions.encoder.layer.10.attention.self.query.bias', 'pyramidions.encoder.layer.10.attention.self.key.weight', 'pyramidions.encoder.layer.10.attention.self.key.bias', 'pyramidions.encoder.layer.10.attention.self.value.weight', 'pyramidions.encoder.layer.10.attention.self.value.bias', 'pyramidions.encoder.layer.10.attention.output.dense.weight', 'pyramidions.encoder.layer.10.attention.output.dense.bias', 'pyramidions.encoder.layer.10.attention.output.LayerNorm.weight', 'pyramidions.encoder.layer.10.attention.output.LayerNorm.bias', 'pyramidions.encoder.layer.10.intermediate.dense.weight', 'pyramidions.encoder.layer.10.intermediate.dense.bias', 'pyramidions.encoder.layer.10.output.dense.weight', 'pyramidions.encoder.layer.10.output.dense.bias', 'pyramidions.encoder.layer.10.output.LayerNorm.weight', 'pyramidions.encoder.layer.10.output.LayerNorm.bias', 'pyramidions.encoder.layer.11.attention.self.query.weight', 'pyramidions.encoder.layer.11.attention.self.query.bias', 'pyramidions.encoder.layer.11.attention.self.key.weight', 'pyramidions.encoder.layer.11.attention.self.key.bias', 'pyramidions.encoder.layer.11.attention.self.value.weight', 'pyramidions.encoder.layer.11.attention.self.value.bias', 'pyramidions.encoder.layer.11.attention.output.dense.weight', 'pyramidions.encoder.layer.11.attention.output.dense.bias', 'pyramidions.encoder.layer.11.attention.output.LayerNorm.weight', 'pyramidions.encoder.layer.11.attention.output.LayerNorm.bias', 'pyramidions.encoder.layer.11.intermediate.dense.weight', 'pyramidions.encoder.layer.11.intermediate.dense.bias', 'pyramidions.encoder.layer.11.output.dense.weight', 'pyramidions.encoder.layer.11.output.dense.bias', 'pyramidions.encoder.layer.11.output.LayerNorm.weight', 'pyramidions.encoder.layer.11.output.LayerNorm.bias'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(rename_roberta_state_dict(pretrained_model.state_dict()), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/lennartkeller/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 775.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/lennartkeller/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-54aedbefd2676f9d.arrow\n",
      "Loading cached processed dataset at /Users/lennartkeller/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-837877adeb715f8d.arrow\n",
      "Loading cached processed dataset at /Users/lennartkeller/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-7a2499041269e44e.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda row: tokenizer(row[\"text\"], truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3562\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 188\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(\"torch\")\n",
    "dataset.rename_column(\"label\", \"labels\")\n",
    "dataset.remove_columns(\"text\")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)[\"test\"]\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      " 50%|█████     | 149/297 [4:04:04<4:02:25, 98.28s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "# I think that the trainer is not using the right padding strategy....\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"pyramid_classif\",\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(padding=\"max_length\", tokenizer=tokenizer),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `PyramidionsForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PyramidionsForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/lennartkeller/Uni/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3562\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 446\n",
      "  0%|          | 1/446 [00:03<22:36,  3.05s/it]\n",
      "  0%|          | 1/446 [00:03<22:36,  3.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6352, 'learning_rate': 2.9932735426008968e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/446 [00:04<16:51,  2.28s/it]\n",
      "  0%|          | 2/446 [00:04<16:51,  2.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6517, 'learning_rate': 2.9865470852017938e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/446 [00:06<13:59,  1.89s/it]\n",
      "  1%|          | 3/446 [00:06<13:59,  1.89s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7654, 'learning_rate': 2.9798206278026905e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 76/446 [3:58:27<19:20:52, 188.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/test_bench.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/test_bench.ipynb#ch0000029?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/trainer.py:1429\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1426'>1427</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1427'>1428</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1428'>1429</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1430'>1431</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1431'>1432</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1432'>1433</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1433'>1434</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1434'>1435</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1435'>1436</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=1436'>1437</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/trainer.py:2058\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2054'>2055</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2056'>2057</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2057'>2058</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2059'>2060</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2060'>2061</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/trainer.py:2090\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2087'>2088</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2088'>2089</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2089'>2090</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2090'>2091</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2091'>2092</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/trainer.py?line=2092'>2093</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py:1286\u001b[0m, in \u001b[0;36mPyramidionsForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1277'>1278</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1278'>1279</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1279'>1280</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1280'>1281</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1281'>1282</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1282'>1283</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1283'>1284</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1285'>1286</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyramidions(\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1286'>1287</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1287'>1288</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1288'>1289</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1289'>1290</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1290'>1291</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1291'>1292</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1292'>1293</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1293'>1294</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1294'>1295</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1295'>1296</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1296'>1297</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=1297'>1298</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py:924\u001b[0m, in \u001b[0;36mPyramidionsModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=914'>915</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=916'>917</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=917'>918</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=918'>919</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=921'>922</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=922'>923</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=923'>924</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=924'>925</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=925'>926</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=926'>927</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=927'>928</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=928'>929</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=929'>930</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=930'>931</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=931'>932</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=932'>933</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=933'>934</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=934'>935</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=935'>936</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=936'>937</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py:601\u001b[0m, in \u001b[0;36mPyramidionsEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=591'>592</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=592'>593</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=593'>594</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=597'>598</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=598'>599</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=599'>600</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=600'>601</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=601'>602</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=602'>603</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=603'>604</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=604'>605</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=605'>606</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=606'>607</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=607'>608</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=608'>609</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=610'>611</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=611'>612</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py:485\u001b[0m, in \u001b[0;36mPyramidionsLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=482'>483</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m attention_mask[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :hidden_states\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)]\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=483'>484</a>\u001b[0m \u001b[39m#print(f\"Attention mask size: {attention_mask.size()}\")\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=484'>485</a>\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=485'>486</a>\u001b[0m     hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=486'>487</a>\u001b[0m     attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=487'>488</a>\u001b[0m     head_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=488'>489</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=489'>490</a>\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=490'>491</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=491'>492</a>\u001b[0m attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=493'>494</a>\u001b[0m \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py:407\u001b[0m, in \u001b[0;36mPyramidionsAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=396'>397</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=397'>398</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=398'>399</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=404'>405</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=405'>406</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=406'>407</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=407'>408</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=408'>409</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=409'>410</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=410'>411</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=411'>412</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=412'>413</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=413'>414</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=414'>415</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=415'>416</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=416'>417</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py:337\u001b[0m, in \u001b[0;36mPyramidionsSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=332'>333</a>\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=334'>335</a>\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=335'>336</a>\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=336'>337</a>\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=338'>339</a>\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennartkeller/Uni/transformers/src/transformers/models/pyramidions/modeling_pyramidions.py?line=339'>340</a>\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/dropout.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/modules/dropout.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/functional.py?line=1276'>1277</a>\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/functional.py?line=1277'>1278</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/transformers_dev/lib/python3.9/site-packages/torch/nn/functional.py?line=1278'>1279</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaked_softmax1(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    \"\"\"\n",
    "    Implementation according to pseudocode in https://arxiv.org/pdf/2010.15552.pdf\n",
    "    \"\"\"\n",
    "    denom = torch.sum(torch.exp(x) - torch.exp(torch.max(x, dim=dim).values.unsqueeze(dim)), dim=dim).unsqueeze(dim)\n",
    "    return torch.exp(x) * (1/denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [0.1, 0.11, 0.112],\n",
    "    [0.1, 0.11, 0.09],\n",
    "    [0.1, 0.11, 0.9]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.4198, -7.0618, -7.1902],\n",
       "        [-3.0111, -3.3122, -2.7100],\n",
       "        [-0.0371, -0.0408, -0.3336]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peaked_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2621],\n",
       "        [1.6922],\n",
       "        [1.7082],\n",
       "        [1.8472],\n",
       "        [1.2688],\n",
       "        [1.6467],\n",
       "        [1.6058],\n",
       "        [1.4219],\n",
       "        [1.5811],\n",
       "        [1.3146],\n",
       "        [1.2749],\n",
       "        [1.8392],\n",
       "        [1.6321],\n",
       "        [1.5127],\n",
       "        [2.5382],\n",
       "        [2.1962]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6622e-01, -8.8706e-01,  6.4478e-01,  1.2643e+00, -5.3926e-01,\n",
       "          1.5800e+00, -1.1016e+00, -1.1637e+00,  1.2379e+00,  2.2707e+00],\n",
       "        [-1.2014e+00, -1.3730e+00, -7.8585e-02, -1.3541e+00, -8.4838e-01,\n",
       "          3.7394e+00,  3.4827e-01,  9.6918e-01, -9.8005e-01, -1.2968e+00],\n",
       "        [ 2.0316e-01, -7.2254e-01, -1.4535e+00,  2.7644e+00, -1.5665e+00,\n",
       "         -1.5133e+00, -4.8308e-01,  3.8107e+00, -2.0801e-01,  1.4802e+00],\n",
       "        [-1.2401e+00, -1.5839e+00, -1.3516e+00, -8.1044e-01,  7.7275e-03,\n",
       "         -1.2508e+00,  1.1936e+00, -1.3482e+00,  4.4948e+00, -2.0269e-01],\n",
       "        [-1.0409e+00, -1.0294e+00,  8.1074e-01, -1.1016e-01,  2.2879e+00,\n",
       "          2.0162e-01,  1.9725e+00,  1.0915e-01, -7.0554e-01, -4.2582e-01],\n",
       "        [-9.5601e-01, -2.2194e-01, -1.2329e+00, -4.2091e-01, -6.2081e-01,\n",
       "         -7.2683e-01,  3.2571e+00, -1.5318e+00,  3.5431e+00, -1.1762e+00],\n",
       "        [-7.0740e-01, -1.3561e+00,  1.9255e+00, -6.9592e-01, -8.2602e-01,\n",
       "          3.3762e+00, -1.4898e-01, -1.1543e+00, -1.0598e+00, -8.4790e-01],\n",
       "        [ 1.4188e-01, -6.3126e-01, -6.6305e-02, -5.5773e-02, -7.6318e-01,\n",
       "         -8.7247e-01, -1.0306e+00, -1.2201e+00,  2.7231e+00,  1.6806e+00],\n",
       "        [ 1.3659e+00,  3.2793e+00,  1.9835e+00,  2.1197e-02, -3.9180e-01,\n",
       "         -9.4405e-01, -9.8586e-01,  1.0291e-01, -1.3501e+00, -1.0523e+00],\n",
       "        [-8.1965e-01,  2.2122e-02,  2.4088e+00, -1.0528e+00, -9.8690e-01,\n",
       "          9.7969e-01,  9.0650e-01, -6.9503e-01, -1.1254e+00, -1.0428e+00],\n",
       "        [-8.3651e-01,  6.5046e-01,  1.4367e-02,  2.2661e-01,  2.3034e+00,\n",
       "          1.5126e-01, -4.6175e-01, -8.1292e-01, -3.7214e-01, -4.2033e-01],\n",
       "        [-7.9468e-01, -1.1997e+00,  2.8436e+00,  6.2037e-02,  4.4524e+00,\n",
       "          7.0316e-02, -1.5607e+00,  2.3107e+00,  1.1777e+00,  1.5927e+00],\n",
       "        [-6.4479e-01, -1.6838e-01,  3.4826e+00, -1.2532e+00, -6.9276e-01,\n",
       "         -1.0939e+00, -1.3808e+00,  6.4697e-01, -1.3678e+00, -8.9319e-01],\n",
       "        [-1.9778e-01, -1.0711e+00, -7.4857e-01,  1.5306e+00,  1.4101e+00,\n",
       "         -7.8178e-01, -1.0295e+00,  5.9887e-02,  3.0263e+00, -9.2584e-01],\n",
       "        [ 1.7073e+00,  1.0119e+01,  1.5855e-01,  2.9744e+00, -2.1499e+00,\n",
       "          3.5193e+00, -2.0291e+00, -1.6836e+00,  4.0394e-01, -6.9536e-01],\n",
       "        [ 2.0513e+00, -1.2422e+00,  2.5743e+00, -1.7177e+00,  6.7942e+00,\n",
       "         -3.2262e-01, -1.8752e+00, -1.7734e+00, -2.0036e+00, -1.7362e+00]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x) - torch.max(x, dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c90e30c2b7961fb97c896d6f081d6fe926e201bd9b65942b4a0d17f0894f15cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('transformers_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
